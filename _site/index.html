<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--><html class="no-js" lang="en"><!--<![endif]-->

<head>
    <meta charset="utf-8">
<title>Clean+Simple Theme</title>
<meta name="description" content="Something...">


<!-- MathJax -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>





<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Clean+Simple Theme">
<meta property="og:description" content="Something...">
<meta property="og:url" content="http://localhost:8000/">
<meta property="og:site_name" content="Clean+Simple Theme">





<link rel="canonical" href="http://localhost:8000/">
<link href="http://localhost:8000/feed.xml" type="application/atom+xml" rel="alternate" title="Clean+Simple Theme Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:8000/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://localhost:8000/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>





    <!-- HEADER IMAGE -->
    <center>
        <span class="main-header-image">
            <a href="/"><img src="/images/header/clean-and-simple-header.jpg"></a>
        </span>
    </center>

    <!-- NAVIGATION -->
    <br><br>
    <center>
        <span class="navigation-bar">
            <a href="/">HOME</a>
            <a href="/archives/">ARCHIVE</a>
            <a href="/tags/">TAGS</a>
            <a href="/about/">ABOUT</a>
            <a href="/feed.xml">RSS</a>
        </span>
    </center> 
</head>
 
<!-- BODY -->
<body id="post-index">
    <!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->

    <!-- MAIN -->
    <div id="main" role="main">
      
<article>
    <!-- POST TITLE -->
    <header>
        <h1 class="entry-title">
            <a href="http://localhost:8000/deep/learning/2017-04-13/paper-reading-dl-watch-move.html" rel="bookmark" title="CVPR2017: Learning Features by Watching Objects Move" itemprop="url">CVPR2017: Learning Features by Watching Objects Move</a>
        </h1>
    </header>
    
    <!-- POST CONTENT -->
    <div class="entry-content">
        <p>其实做法非常简单，就是利用视频的光流信息来做前景分割，然后用模型去学习前景分割的结果。结果发现，虽有利用光流信息得到的分割结果噪声很大（pseudo ground truth），但是训练出来的模型确得到很好的特征，可以用于物体检测等。</p>

<p>本文最值得学习的是写作的手法，虽然方法非常简单，但是写作上比较有水平。</p>

<p>1.在Introduction部分，把这种利用间接非监督任务（pretext task）来学习好的特征的方法进行了归纳。作者归纳了以前的四种pretext task：Example pretext tasks include reconstruct-ing the input [3, 18, 41], predicting the pixels of the next frame in a video stream [15], metric learning on object track endpoints [43], temporally ordering shuffled frames from a video [26], and spatially ordering patches from a static im- age [6, 27]. 
而本文是一种新的pretext task。</p>

<p>2.展开讲自己的方法之前，先讲Evaluating Feature Representations的方法。指出了这个问题的重要性，以及以前评估方法的局限性，并提出了自己的三种更general的评估方法。有点测试先行的意思。</p>

<p>3.对比实验。首先直接使用ImageNet的语义分割模型（无label分割），说明无label分类的模型确实学到可可以用于其他任务的特征，而本文则是无监督的方式得到的类似特征。</p>

<p>其他方面没有太多好说的。使用光流得到pseudo ground truth的方法都是既有的，而且分割的噪声是更大的，但是本文证明了噪声对特征学习影响不大。</p>

<center>
    <img src="http://7xkiab.com1.z0.glb.clouddn.com/blog/image/2017-04-13-watch-move.jpg" width="400" />
</center>


    </div>
    
    <!--- DIVIDING LINE -->
    <hr>
    
    <!-- POST TAGS -->
    <div class="inline-tags">
        <span>
        
            <a href="/tags/#Paper Reading">#Paper Reading&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Deep Learning">#Deep Learning&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Unsupervised Learning">#Unsupervised Learning&nbsp;&nbsp;&nbsp;</a>
        
        </span>
    </div>
    
    <br>
    
    <!-- POST DATE -->
    <div class="post-date">
        <span class="entry-date date published updated">
            <time datetime="2017-04-13T00:00:00+08:00">
                <a href="http://localhost:8000/deep/learning/2017-04-13/paper-reading-dl-watch-move.html">April 13, 2017</a>
            </time>
        </span>
    </div>
    
</article>

<article>
    <!-- POST TITLE -->
    <header>
        <h1 class="entry-title">
            <a href="http://localhost:8000/deep/learning/2017-04-11/paper-reading-dl-res-next.html" rel="bookmark" title="Paper Reading: ResNext--Aggregated Residual Transformations for Deep Neural Networks" itemprop="url">Paper Reading: ResNext--Aggregated Residual Transformations for Deep Neural Networks</a>
        </h1>
    </header>
    
    <!-- POST CONTENT -->
    <div class="entry-content">
        <h2 id="简介">简介</h2>
<p>可以认为是ResNet的升级版。原理非常简单，一张图即可搞明白：</p>

<center>
    <img src="http://img.blog.csdn.net/20170411142838359?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="300" />
</center>

<p>这种思想可以有三种实现形式：</p>

<center>
    <img src="http://img.blog.csdn.net/20170411143026219?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="600" />
</center>

<p>作者实验发现，三种形式效果差不多，考虑计算性能，选择第三种。</p>

<p>作者提出一个概念：把上述building block中除了short-cut以外的支路数量称为cardinality（基数）。作者把cardinality和depth、width两个概念并列，并在实验中证明增加cardinality比增加depth或width更加有效。</p>

<center>
    <img src="http://img.blog.csdn.net/20170411143348329?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="350" />
</center>

<p>在FLOP同样增加到2倍的情况下，增加cardinality的方法获得了最好的结果（top5:5.3%）。而且即使在FLOP只有1/2的情况下，ResNeXt也比ResNet效果好。</p>


    </div>
    
    <!--- DIVIDING LINE -->
    <hr>
    
    <!-- POST TAGS -->
    <div class="inline-tags">
        <span>
        
            <a href="/tags/#Paper Reading">#Paper Reading&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Deep Learning">#Deep Learning&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Network Building Block">#Network Building Block&nbsp;&nbsp;&nbsp;</a>
        
        </span>
    </div>
    
    <br>
    
    <!-- POST DATE -->
    <div class="post-date">
        <span class="entry-date date published updated">
            <time datetime="2017-04-11T00:00:00+08:00">
                <a href="http://localhost:8000/deep/learning/2017-04-11/paper-reading-dl-res-next.html">April 11, 2017</a>
            </time>
        </span>
    </div>
    
</article>

<article>
    <!-- POST TITLE -->
    <header>
        <h1 class="entry-title">
            <a href="http://localhost:8000/deep/learning/2017-04-10/paper-reading-driving-end-to-end.html" rel="bookmark" title="CVPR2017: End-to-end Learning of Driving Models from Large-scale Video Datasets" itemprop="url">CVPR2017: End-to-end Learning of Driving Models from Large-scale Video Datasets</a>
        </h1>
    </header>
    
    <!-- POST CONTENT -->
    <div class="entry-content">
        <h2 id="简介">简介</h2>
<p>采用基于感知的策略学习（perception-based policies）来完成复杂的自动驾驶行为，正在称为计算机视觉的一个重要研究方向。作者认为，虽然基于规则的方法取得了一定的成果，但是基于学习的方法才是终极方案，尤其是在处理复杂少见场景以及需要和多个agent交互作用的情况。在这个方向上已经有不少不做，例如ALVINN等。这些工作把问题定义成从图像像素到操作的映射关系，导致训练上依赖于标定的操作。而本文采用的是无需标定的数据，这样就可以利用大量众包的视频数据，基本只需要一个行车记录仪（+速度和角速度记录）。在问题定义上，把驾驶过程定义成从图像到egomotion的映射关系，而egomotion的groundtruth可以通过速度和角速度记录记录得到。作者采用了FCN-LSTM的网络框架，并把这种网络称为：Deep Generic Driving Networks。</p>

<h2 id="原理">原理</h2>

<p>具体问题定义如下式：</p>

<center>
    <img src="http://img.blog.csdn.net/20170410142523524?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="120" />
</center>

<p>其中S表示当前和历史的状态信息，包括：历史上见过的画面以及egomotion信息。A表示下一时刻应该采取的egomotion的集合，R表示对应的分数。egomotion有两种描述方式：一种是离散型，比如{直行，停车，左转，右转}；另一种是连续型：用一个二维的速度向量表示，实际训练时，对该二维向量离散成多个bin，进行mulit-nominal学习。这种问题定义和NLP中的语言模型非常相似，所以该模型也称为“驾驶模型”。</p>

<p>FCN-LSTM如下图：</p>

<center>
    <img src="http://img.blog.csdn.net/20170410142733900?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="500" />
</center>

<p>其中，FCN采用的在ImageNet上pretrain过的AlexNet，去掉了pooling5，并对fc6和fc7采用dilated convolutions。作者指出FCN比直接用CNN效果要好，是因为FCN保留了更多的spatial信息。</p>

<p>Loss方面，作者采用了：</p>

<center>
    <img src="http://img.blog.csdn.net/20170410143725939?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="250" />
</center>

<p>对于Accuracy，作者采用：</p>

<center>
    <img src="http://img.blog.csdn.net/20170410143854610?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="120" />
</center>

<h2 id="实验">实验</h2>
<p>模型效果：</p>

<center>
    <img src="http://img.blog.csdn.net/20170410144007704?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="400" />
</center>

<p>加语义分割任务作为side-task的对比实验效果：</p>

<center>
    <img src="http://img.blog.csdn.net/20170410144116179?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="400" />
</center>

<p>图中，a是不用side-task的方式，b是只用语义分割结果，c是用FCN+side-task的情况。示例中一个是远处有红灯，一个是前方车辆尾灯刹车灯亮，这两种情况中只有c较好地解决了。作者指出，对于只用语义分割的情况可能会丢失一些content信息例如灯的颜色，而加入side-task可以改进一些细微的和不常见的情况下的性能。</p>

<h2 id="结论">结论</h2>
<p>主要是三个贡献：</p>

<ol>
  <li>重新定义了问题，采用了图像到egomotion的映射。</li>
  <li>采用了FCN-LSTM的架构。</li>
  <li>采用语义分割作为side-task，并提高了模型性能。</li>
</ol>


    </div>
    
    <!--- DIVIDING LINE -->
    <hr>
    
    <!-- POST TAGS -->
    <div class="inline-tags">
        <span>
        
            <a href="/tags/#Paper Reading">#Paper Reading&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Deep Learning">#Deep Learning&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Autonomous Driving">#Autonomous Driving&nbsp;&nbsp;&nbsp;</a>
        
        </span>
    </div>
    
    <br>
    
    <!-- POST DATE -->
    <div class="post-date">
        <span class="entry-date date published updated">
            <time datetime="2017-04-10T00:00:00+08:00">
                <a href="http://localhost:8000/deep/learning/2017-04-10/paper-reading-driving-end-to-end.html">April 10, 2017</a>
            </time>
        </span>
    </div>
    
</article>

<article>
    <!-- POST TITLE -->
    <header>
        <h1 class="entry-title">
            <a href="http://localhost:8000/deep/learning/2017-04-09/paper-reading-dl-neural-art.html" rel="bookmark" title="Paper Reading: A Neural Algorithm of Artistic Style" itemprop="url">Paper Reading: A Neural Algorithm of Artistic Style</a>
        </h1>
    </header>
    
    <!-- POST CONTENT -->
    <div class="entry-content">
        <h2 id="简介">简介</h2>
<p>解决的是一个有趣的问题，即给两幅图C(content)和S(style)，保留C的content，保留S的style，得到一副新的图像。例如：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408191713226?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="400" />
</center>

<p>CNN的不同层编码了不同抽象层次的信息，可以借助不同层的Feature Map来对生成图像的content和style进行约束，见下图：</p>

<center>
    <img src="http://img.blog.csdn.net/20170409143704521?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="500" />
</center>

<h2 id="neural-art">Neural Art</h2>

<p>本文的原理非常简单。</p>

<p>设content图像为C，待生成的图像为X。先将X设置为随机噪声，然后作为VGG的输入，C同样也过一遍VGG，这样对于某个层l，两者都会得到feature map，分为设置为P和F，则content loss定义为：</p>

<center>
    <img src="http://img.blog.csdn.net/20170409144031447?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="200" />
</center>

<p>固定住两个支路的所有参数，除了X本身。对X本身进行梯度下降，得到的X即是和C保持了content一致性（一定意义上）。</p>

<p>对于style，loss定义稍微改变一点，使用了Gram matrix。对于某个feature map F，Gram matrix为：</p>

<center>
    <img src="http://img.blog.csdn.net/20170409144402733?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="120" />
</center>

<p>即gram matrix的维度为NxN，N为channels，每个元素的值是对第i个channel和第j个channel求内积得到的。实际上是协方差矩阵。个人理解是通过gram matrix，是的约束更多是考虑统计特征，而不是localization相关的content。</p>

<p>style loss定义为：</p>

<center>
    <img src="http://img.blog.csdn.net/20170409145138541?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="160" />
</center>

<p>综合起来，loss为：</p>

<center>
    <img src="http://img.blog.csdn.net/20170409145446393?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="280" />
</center>

<h2 id="实验">实验</h2>
<p>一些对比实验结果如下：</p>

<center>
    <img src="http://img.blog.csdn.net/20170409145323346?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="600" />
</center>

<p>包括对不同层的选用，以及两个loss之前权重的选择等。</p>

<h2 id="结论">结论</h2>

<p>使用了非常简单粗暴的方法，利用物体检测任务训练得到的模型（VGG）的特征，即可实现不错的图像风格转换的效果。其中，在style loss中对gram matrix的使用值得借鉴。</p>

<p>不足：这种方式需要对生成图像进行梯度下降，而VGG模型不小，耗时应该是一个主要问题。</p>


    </div>
    
    <!--- DIVIDING LINE -->
    <hr>
    
    <!-- POST TAGS -->
    <div class="inline-tags">
        <span>
        
            <a href="/tags/#Paper Reading">#Paper Reading&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Deep Learning">#Deep Learning&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Style Transfer">#Style Transfer&nbsp;&nbsp;&nbsp;</a>
        
        </span>
    </div>
    
    <br>
    
    <!-- POST DATE -->
    <div class="post-date">
        <span class="entry-date date published updated">
            <time datetime="2017-04-09T00:00:00+08:00">
                <a href="http://localhost:8000/deep/learning/2017-04-09/paper-reading-dl-neural-art.html">April 09, 2017</a>
            </time>
        </span>
    </div>
    
</article>

<article>
    <!-- POST TITLE -->
    <header>
        <h1 class="entry-title">
            <a href="http://localhost:8000/deep/learning/2017-04-08/paper-reading-dl-learning-to-learn.html" rel="bookmark" title="CVPR2017: Learning to learn by gradient descent by gradient descent" itemprop="url">CVPR2017: Learning to learn by gradient descent by gradient descent</a>
        </h1>
    </header>
    
    <!-- POST CONTENT -->
    <div class="entry-content">
        <h2 id="简介">简介</h2>
<p>昨天DeepMind开源了基于TensorFlow的深度学习库——<a href="https://deepmind.com/blog/open-sourcing-sonnet/">Sonnet</a>。DeepMind之前发布的learning to learn的<a href="https://github.com/deepmind/learning-to-learn">代码</a>实际上就是基于Sonnet实现的。</p>

<p>本文解决的是优化算法的学习问题。具体来说，机器学习中我们经常可以把优化目标表示成：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408184112158?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="150" />
</center>

<p>标准的优化过程是更新如下过程：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408184217208?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="180" />
</center>

<p>这样的做法只考虑了一阶导数而不考虑二阶导数，显然会损失性能。典型的方法是通过Hessian matrix等方法来弥补。而现代的更多的方法则是针对具体类问题，比如深度学习领域经常使用的方法有：momentum、Rprop、Adagrad、RMSprop、ADAM。使用这些方法的一个最大问题是：在不同的问题上需要选择不同的优化方法。</p>

<p>本文试图寻找一个大一统的方案：采用RNN来学习参数更新过程，而不是手动去选择不同的优化方法。如下式：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408184811304?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="180" />
</center>

<p>这样问题变成如何去学习函数g。作者把该问题称为：Learning to learn。</p>

<h2 id="learning-to-learn">Learning to learn</h2>
<p>作者采用RNN来做Learning to learn，即学习每次迭代的参数更新。显然，采用RNN这种带记忆功能的网络来学习更符合这里的需求（克服传统方法只考虑一阶导数的缺点）。理想的优化目标为：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408185028530?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="160" />
</center>

<p>即希望这样的参数更新更有利于f的下降。</p>

<p>对于RNN，假设选择记忆的长度为T，则优化目标为：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408185413135?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="400" />
</center>

<p>其中m表示RNN，h表示其隐状态。本文的实验中，所有的权重w均取1。</p>

<p>考虑到如果同时学习所有参数，会导致RNN的参数量过大，所以采用了Coordinatewise的方法。可以认为是把参数划分成不同的等分，然后共享参数，但是不共享隐状态。如下图：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408185747949?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="300" />
</center>

<h2 id="实验">实验</h2>
<p>作者做了四个实验：</p>

<ul>
  <li>Quadratic functions</li>
  <li>Training a small neural network on MNIST</li>
  <li>Training a convolutional network on CIFAR-10</li>
  <li>Neural Art</li>
</ul>

<p>均证明了该方法比其他优化方法更好：收敛快且loss更低，泛华性能也很好。</p>

<p>例如，Neural Art实验，loss变化曲线：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408190041014?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="400" />
</center>

<p>效果：</p>

<center>
    <img src="http://img.blog.csdn.net/20170408190120124?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd3VsdzE5OTA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="600" />
</center>

<h2 id="结论">结论</h2>
<p>用Learning to learn的方法来作为优化算法，比其他手工选择的优化算法都要好！</p>

    </div>
    
    <!--- DIVIDING LINE -->
    <hr>
    
    <!-- POST TAGS -->
    <div class="inline-tags">
        <span>
        
            <a href="/tags/#Paper Reading">#Paper Reading&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Deep Learning">#Deep Learning&nbsp;&nbsp;&nbsp;</a>
        
            <a href="/tags/#Learning to Learn">#Learning to Learn&nbsp;&nbsp;&nbsp;</a>
        
        </span>
    </div>
    
    <br>
    
    <!-- POST DATE -->
    <div class="post-date">
        <span class="entry-date date published updated">
            <time datetime="2017-04-08T00:00:00+08:00">
                <a href="http://localhost:8000/deep/learning/2017-04-08/paper-reading-dl-learning-to-learn.html">April 08, 2017</a>
            </time>
        </span>
    </div>
    
</article>


<div class="pagination">
<center>
<div class="pagination">
    

        
            <a href="http://localhost:8000/page2/">OLDER→</a>
        
    
        
                                         
        
    
</div>
</center>
    </div>
    </div>       
</body>

<!-- FOOTER -->  
<footer>  
    <div class="footer-wrapper">
        <footer role="contentinfo">
            <span>
    &copy; 2017 YOUR NAME HERE.<br>Powered by <a target="_blank" href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a target="_blank" href="https://github.com/nathanrooy/Clean-and-Simple-Jekyll-Theme">Clean+Simple</a> theme.
</span>

        </footer>
    </div>   
</footer>
</html>
